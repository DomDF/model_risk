@techreport{BritishStandardsInstitution1987,
author = {{British Standards Institution}},
file = {::},
isbn = {0580159175},
title = {{BS 2633 - Specification for
Class I arc welding of
ferritic steel pipework
for carrying fluids}},
year = {1987}
}
@techreport{BritishStandardsInstitute2021,
author = {{British Standards Institute}},
file = {:Users/ddifrancesco/Library/Application Support/Mendeley Desktop/Downloaded/British Standards Institute - 2021 - BS EN 13445 - Unfired pressure vessels.pdf:pdf},
isbn = {9780539179477},
title = {{BS EN 13445 - Unfired pressure vessels}},
year = {2021}
}
@techreport{BritishStandardsInstitute2009,
author = {{British Standards Institute}},
file = {::},
isbn = {9780580580000},
title = {{BS EN 1011 - Welding - Recommendations for welding of metallic materials}},
year = {2009}
}
@techreport{BritishStandardsInstitute2004,
author = {{British Standards Institute}},
file = {::},
isbn = {0580454614},
title = {{BS EN 14015 - Specification for the design and manufacture of site built,
vertical, cylindrical, flat-bottomed, above ground, welded, steel
tanks for the storage of liquids at ambient temperature and
above}},
year = {2004}
}
@article{Lu2020,
abstract = {Concept drift describes unforeseeable changes in the underlying distribution of streaming data over time. Concept drift research involves the development of methodologies and techniques for drift detection, understanding and adaptation. Data analysis has revealed that machine learning in a concept drift environment will result in poor learning results if the drift is not addressed. To help researchers identify which research topics are significant and how to apply related techniques in data analysis tasks, it is necessary that a high quality, instructive review of current research developments and trends in the concept drift field is conducted. In addition, due to the rapid development of concept drift in recent years, the methodologies of learning under concept drift have become noticeably systematic, unveiling a framework which has not been mentioned in literature. This paper reviews over 130 high quality publications in concept drift related research areas, analyzes up-to-date developments in methodologies and techniques, and establishes a framework of learning under concept drift including three main components: concept drift detection, concept drift understanding, and concept drift adaptation. This paper lists and discusses 10 popular synthetic datasets and 14 publicly available benchmark datasets used for evaluating the performance of learning algorithms aiming at handling concept drift. Also, concept drift related research directions are covered and discussed. By providing state-of-the-art knowledge, this survey will directly support researchers in their understanding of research developments in the field of learning under concept drift.},
archivePrefix = {arXiv},
arxivId = {2004.05785},
author = {Lu, Jie and Liu, Anjin and Dong, Fan and Gu, Feng and Gama, Joao and Zhang, Guangquan},
doi = {10.1109/TKDE.2018.2876857},
eprint = {2004.05785},
file = {::},
month = {apr},
title = {{Learning under Concept Drift: A Review}},
url = {http://arxiv.org/abs/2004.05785 http://dx.doi.org/10.1109/TKDE.2018.2876857},
year = {2020}
}
@inproceedings{Altmeyer2023,
abstract = {We present CounterfactualExplanations.jl: a package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box models in Julia. CE explain how inputs into a model need to change to yield specific model predictions. Explanations that involve realistic and actionable changes can be used to provide AR: a set of proposed actions for individuals to change an undesirable outcome for the better. In this article, we discuss the usefulness of CE for Explainable Artificial Intelligence and demonstrate the functionality of our package. The package is straightforward to use and designed with a focus on customization and extensibility. We envision it to one day be the go-to place for explaining arbitrary predictive models in Julia through a diverse suite of counterfactual generators.},
author = {Altmeyer, Patrick and {Van Deursen}, Arie and Liem, Cynthia C S},
booktitle = {JuliaCon Proceedings},
doi = {doi.org/10.21105/jcon.00130},
file = {::},
keywords = {Algorithmic Recourse,Counterfactual Explana-tions,Explainable Artificial Intelligence,Julia},
pages = {130},
title = {{Explaining Black-Box Models through Counterfactuals}},
year = {2023}
}
@inproceedings{Abid2021,
abstract = {Understanding and explaining the mistakes made by trained models is critical to many machine learning objectives, such as improving robustness, addressing concept drift, and mitigating biases. However, this is often an ad hoc process that involves manually looking at the model's mistakes on many test samples and guessing at the underlying reasons for those incorrect predictions. In this paper, we propose a systematic approach, conceptual counterfactual explanations (CCE), that explains why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). We base CCE on two prior ideas: counterfactual explanations and concept activation vectors, and validate our approach on well-known pretrained models, showing that it explains the models' mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample. On two challenging medical applications, CCE generated useful insights, confirmed by clinicians, into biases and mistakes the model makes in real-world settings.},
archivePrefix = {arXiv},
arxivId = {2106.12723},
author = {Abid, Abubakar and Yuksekgonul, Mert and Zou, James},
booktitle = {39th International Conference on Machine Learning, PMLR},
eprint = {2106.12723},
file = {::},
month = {jun},
pages = {66--88},
title = {{Meaningfully Debugging Model Mistakes using Conceptual Counterfactual Explanations}},
url = {http://arxiv.org/abs/2106.12723},
year = {2021}
}
@article{Totino2023,
abstract = {In the last few years, extracting, analyzing and classifying welding defects in radiographic images received a great deal of attention in several industry manufacturing. Nowadays, computer vision affords considerable accuracy in many practical applications, but making automatic processes approachable also in this field is still a challenge. As an example, Convolutional Neural Networks (CNNs) are widely recognized as efficient and accurate classification structures, but, due to the limited availability of specific datasets, training a CNN to classify welding defects is not trivial. This paper presents a new dataset collecting 24,407 radiographic images related to several classes of welding defects: lack of penetration, cracks, porosity and no defect. The proposed dataset of welding defects in radiographic images is released freely to the research community. As an example of application, the dataset has been used to train a customized version of the SqueezeNet CNN obtaining a test accuracy higher than 93%.},
author = {Totino, Benito and Spagnolo, Fanny and Perri, Stefania},
doi = {10.53375/ijecer.2023.320},
file = {::},
journal = {International Journal of Electrical and Computer Engineering Research},
month = {mar},
number = {1},
pages = {13--17},
publisher = {International Journal of Electrical and Computer Engineering Research},
title = {{RIAWELC: A Novel Dataset of Radiographic Images for Automatic Weld Defects Classification}},
volume = {3},
year = {2023}
}
@techreport{PortsdownWest2014,
author = {{Portsdown West}, Dstl},
file = {::},
title = {{The probabilistic elicitation of subjective data UK OFFICIAL}},
year = {2014}
}
@article{Gelman2016,
abstract = {In summary, I agree with most of the ASA's statement on p-values but I feel that the problems are deeper, and that the solution is not to reform p-values or to replace them with some other statistical summary or threshold, but rather to move toward a greater acceptance of uncertainty and embracing of variation.},
author = {Gelman, Andrew},
doi = {10.1080/00031305.2016.1154108},
issn = {0003-1305},
journal = {The American Statistician},
pages = {1--2},
title = {{The problems with p-values are not just with p-values}},
year = {2016}
}
@techreport{DNV2021a,
author = {DNV},
title = {{DNV-SE-0474 Risk based verification}},
url = {https://standards.dnv.com/},
year = {2021}
}
@misc{HealthandSafetyExecutiveHSE,
author = {{Health and Safety Executive}},
title = {{ALARP "at a glance"}},
url = {https://www.hse.gov.uk/enforce/expert/alarpglance.htm},
urldate = {2024-07-01}
}
@book{Sutton2020,
author = {Sutton, Richard S. and Barto, Andrew G.},
booktitle = {Universitas Nusantara PGRI Kediri},
edition = {Second},
isbn = {9780262039246},
publisher = {MIT press},
title = {{Reinforcement Learning An Introduction}},
volume = {01},
year = {2020}
}
@article{Gelman2020b,
abstract = {Every philosophy has holes, and it is the responsibility of proponents of a philosophy to point out these problems. Here are a few holes in Bayesian data analysis: (1) the usual rules of conditional probability fail in the quantum realm, (2) flat or weak priors lead to terrible inferences about things we care about, (3) subjective priors are incoherent, (4) Bayes factors fail in the presence of flat or weak priors, (5) for Cantorian reasons we need to check our models, but this destroys the coherence of Bayesian inference. Some of the problems of Bayesian statistics arise from people trying to do things they shouldn't be trying to do, but other holes are not so easily patched. In particular, it may be a good idea to avoid flat, weak, or conventional priors, but such advice, if followed, would go against the vast majority of Bayesian practice and requires us to confront the fundamental incoherence of Bayesian inference. This does not mean that we think Bayesian inference is a bad idea, but it does mean that there is a tension between Bayesian logic and Bayesian workflow which we believe can only be resolved by considering Bayesian logic as a tool, a way of revealing inevitable misfits and incoherences in our model assumptions, rather than as an end in itself.},
archivePrefix = {arXiv},
arxivId = {2002.06467},
author = {Gelman, Andrew and Yao, Yuling},
doi = {10.1088/1361-6471/abc3a5},
eprint = {2002.06467},
issn = {0954-3899},
journal = {Journal of Physics G: Nuclear and Particle Physics},
number = {1},
pages = {014002},
publisher = {IOP Publishing},
title = {{Holes in bayesian statistics}},
volume = {48},
year = {2020}
}
@book{AmericanPetroleumInstitute2016,
author = {{American Petroleum Institute}},
edition = {Third Edit},
title = {{Risk-Based Inspection, API RP 580}},
year = {2016}
}
@book{Gelman2014,
abstract = {Now in its third edition, this classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data Analysis, Third Edition continues to take an applied approach to analysis using up-to-date Bayesian methods. The authors—all leaders in the statistics community—introduce basic concepts from a data-analytic perspective before presenting advanced methods. Throughout the text, numerous worked examples drawn from real applications and research emphasize the use of Bayesian inference in practice. New to the Third Edition Four new chapters on nonparametric modeling Coverage of weakly informative priors and boundary-avoiding priors Updated discussion of cross-validation and predictive information criteria Improved convergence monitoring and effective sample size calculations for iterative simulation Presentations of Hamiltonian Monte Carlo, variational Bayes, and expectation propagation New and revised software code The book can be used in three different ways. For undergraduate students, it introduces Bayesian inference starting from first principles. For graduate students, the text presents effective current approaches to Bayesian modeling and computation in statistics and related fields. For researchers, it provides an assortment of Bayesian methods in applied statistics. Additional materials, including data sets used in the examples, solutions to selected exercises, and software instructions, are available on the book's web page.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gelman, Andrew and Carlin, John B B and Stern, Hal S S and Rubin, Donald B B},
doi = {10.1007/s13398-014-0173-7.2},
edition = {3rd},
editor = {Dominici, Francesca and Faraway, Julian J. and Tanner, Martin and Zidek, Jim},
eprint = {arXiv:1011.1669v3},
isbn = {9781439840955},
issn = {1467-9280},
pages = {675},
pmid = {25052830},
publisher = {Chapman & Hall / CRC},
title = {{Bayesian Data Analysis}},
year = {2014}
}
@book{AmericanPetroleumInstitute2008,
abstract = {This publication provides quantitative procedures to establish an inspection program using risk-based methods for pressurized fixed equipment, including pressure vessel, piping, tankage, pressure relief devices, and heat exchanger tube bundles. This document is to be used in conjunction with API 580, which provides guidance on developing a riskbased inspection program for fixed equipment in the refining and petrochemical, and chemical process plants. The intent of these publications is for API 580 to introduce the principals and present minimum general guidelines for RBI while this publication provides quantitative calculation methods to determine an inspection plan using a risk-based methodology. The API Risk-Based Inspection (API RBI) methodology may be used to manage the overall risk of a plant by focusing inspection efforts on the process equipment with the highest risk. API RBI provides the basis for making informed decisions on inspection frequency, the extent of inspection, and the most suitable type of NDE. In most processing plants, a large percent of the total unit risk will be concentrated in a relatively small percent of the equipment items. These potential high-risk components may require greater attention, perhaps through a revised inspection plan. The cost of the increased inspection effort may sometimes be offset by reducing excessive inspection efforts in the areas identified as having lower risk. Shall: As used in a standard, “shall” denotes a minimum requirement in order to conform to the specification. Should: As used in a standard, “should” denotes a recommendation or that which is advised but not required in order to conform to the specification. May: As used in a standard, “may” indicates recommendations that are optional. Nothing contained in any API publication is to be construed as granting any right, by implication or otherwise, for the manufacture, sale, or use of any method, apparatus, or product covered by letters patent. Neither should anything contained in the publication be construed as insuring anyone against liability for infringement of letters patent. This document was produced under API standardization procedures that ensure appropriate notification and participation in the developmental process and is designated as an API standard. Questions concerning the interpretation of the content of this publication or comments and questions concerning the procedures under which this publication was developed should be directed in writing to the Director of Standards, American Petroleum Institute, 1220 L Street, N.W., Washington, D.C. 20005. Requests for permission to reproduce or translate all or any part of the material published herein should also be addressed to the director. Generally, API standards are reviewed and revised, reaffirmed, or withdrawn at least every five years. A one-time extension of up to two years may be added to this review cycle. Status of the publication can be ascertained from the API Standards Department, telephone (202) 682-8000. A catalog of API publications and materials is published annually by API, 1220 L Street, N.W., Washington, D.C. 20005. Suggested revisions are invited and should be submitted to the Standards Department, API, 1220 L Street, NW, Washington, D.C. 20005, standards@api.org.},
author = {{American Petroleum Institute}},
booktitle = {API Recommended Practice 581},
isbn = {978-664-8806-69-4},
title = {{Risk-Based Inspection Technology, API RP 581}},
year = {2008}
}
@article{Ainsworth2008,
abstract = {This paper first briefly summarises the existing methods in the low-temperature fracture assessment procedure, R6, and the high-temperature procedure, R5, for treating the effects of secondary stresses on structural integrity. Recently, there have been a number of developments, which identify the way forward for these procedures. A modified J-integral definition has been derived, which is path independent for cases of proportional and non-proportional loading and is ideal for evaluating the crack driving force for defects in secondary and residual stress fields. Results of finite element analysis are presented that show that the use of the modified J-integral can lead to a lower crack driving force for secondary stresses than current simplified R6 methods. More detailed calculations have assessed the effects on fracture of a slowly growing crack and constraint effects associated with secondary stresses. Preliminary results are presented, showing the long-term potential of more advanced methods in providing significant benefits in structural integrity assessments. For high-temperature applications, the paper presents methods for calculating the relaxation of secondary stresses due to both creep strain and creep crack growth, extending current methods in R5 that only allow for relaxation due to creep strain. Related work addressing the combined effects of plasticity and creep on relaxation of the crack tip fields is also presented and the results are illustrated for a typical geometry and loading. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Ainsworth, R. A. and Hooton, D. G.},
doi = {10.1016/j.ijpvp.2007.10.003},
isbn = {0308-0161},
issn = {03080161},
journal = {International Journal of Pressure Vessels and Piping},
keywords = {Creep,Fracture,R5,R6,Residual stress,Secondary stress},
title = {{R6 and R5 procedures: The way forward}},
year = {2008}
}
@article{Saidi2025,
author = {Saidi, Pouria and Dasarathy, Gautam and Berisha, Visar},
doi = {10.1016/j.patter.2025.101185},
file = {:Users/ddifrancesco/Downloads/main.pdf:pdf},
issn = {2666-3899},
journal = {Patterns},
number = {4},
pages = {101185},
publisher = {The Author(s)},
title = {{Article Unraveling overoptimism and publication bias in ML- driven science Unraveling overoptimism and publication bias in ML-driven science}},
url = {https://doi.org/10.1016/j.patter.2025.101185},
volume = {6},
year = {2025}
}
@techreport{DNV2021,
author = {DNV},
file = {:Users/ddifrancesco/Downloads/[DNV-RP-N101] Risk management in marine and subsea operations - edition Sep, 2019 (amended Sep, 2021).pdf:pdf},
title = {{Risk management in marine and subsea operations}},
year = {2021}
}
@techreport{NASA2020,
author = {NASA},
file = {:Users/ddifrancesco/Downloads/prc-6503-current.pdf:pdf},
number = {January},
pages = {1--10},
title = {{PRC-6503: Process Specification for Radiographic Inspection}},
year = {2020}
}
@article{NASA2008,
author = {NASA},
file = {:Users/ddifrancesco/Downloads/nasa_std_5009.pdf:pdf},
number = {I},
pages = {1--28},
title = {{NASA-STD-5009: Nondestructive Evaluation Requirements for Fracture-Critical Metallic Components}},
year = {2008}
}
@article{DiFrancesco2025,
abstract = {Novel methods of data collection and analysis can enhance traditional risk management practices that rely on expert engineering judgment and established safety records, specifically when key conditions are met: Analysis is linked to the decisions it is intended to support, standards and competencies remain up to date, and assurance and verification activities are performed. This article elaborates on these conditions. The reason engineers are required to perform calculations is to support decision-making. Since humans are famously weak natural statisticians, rather than ask stakeholders to implicitly assimilate data, and arrive at a decision, we can instead rely on subject matter experts to explicitly define risk management decision problems. The results of engineering calculation can then also communicate which interventions (if any) are considered to be risk-optimal. It is also proposed that the next generation of engineering standards should learn from the success of open source software development in community building. Interacting with open datasets and code can promote engagement, identification (and resolution) of errors, training and ultimately competence. Finally, the profession's tradition of independent verification should also be applied to the complex models that will increasingly contribute to the safety of the built environment. Model assurance will be required to keep pace with model development to identify suitable use cases as adequately safe. These are considered to be increasingly important components in ensuring that methods of data-centric engineering can be safely and appropriately adopted in industry.},
author = {{Di Francesco}, Domenic},
doi = {10.1017/dce.2025.10},
file = {::},
issn = {26326736},
journal = {Data-Centric Engineering},
keywords = {assurance,computational statistics,machine learning,risk management},
month = {feb},
publisher = {Cambridge University Press},
title = {{Risk management in the era of data-centric engineering}},
volume = {6},
year = {2025}
}
@inproceedings{Carr2023,
abstract = {Safe exploration is a common problem in reinforcement learning (RL) that aims to prevent agents from making disastrous decisions while exploring their environment. A family of approaches to this problem assume domain knowledge in the form of a (partial) model of this environment to decide upon the safety of an action. A so-called shield forces the RL agent to select only safe actions. However, for adoption in various applications, one must look beyond enforcing safety and also ensure the applicability of RL with good performance. We extend the applicability of shields via tight integration with state-of-the-art deep RL, and provide an extensive, empirical study in challenging, sparse-reward environments under partial observability. We show that a carefully integrated shield ensures safety and can improve the convergence rate and final performance of RL agents. We furthermore show that a shield can be used to bootstrap state-of-the-art RL agents: they remain safe after initial learning in a shielded setting, allowing us to disable a potentially too conservative shield eventually.},
archivePrefix = {arXiv},
arxivId = {2204.00755},
author = {Carr, Steven and Jansen, Nils and Junges, Sebastian and Topcu, Ufuk},
booktitle = {The Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)},
eprint = {2204.00755},
file = {::},
keywords = {SRAI: Formal Methods for AI Systems: General,SRAI: Safe Decision Making Under Uncertainty: Gene,SRAI: Safe Learning: General},
title = {{Safe Reinforcement Learning via Shielding under Partial Observability}},
url = {www.aaai.org},
year = {2023}
}
@misc{Bates2024,
author = {Bates, Lisa K.},
booktitle = {Planning Theory and Practice},
doi = {10.1080/14649357.2024.2423570},
file = {::},
issn = {1470000X},
publisher = {Routledge},
title = {{A Computer Must Never Make a Planning Decision}},
year = {2024}
}
@techreport{Letter11_7,
author = {{Board of Governors of the Federal Reserve System Office of the Comptroller of the Currency}},
file = {::},
keywords = {Basel II,advanced approach,back testing,conceptual soundness,developmental evidence,effective challenge,governance,model,model development,model risk,ongoing monitoring,outcomes analysis,pillar 2,risk management,sensitivity analysis,stress testing,validation,vendor models},
title = {{SR Letter 11-7 Supervisory Guidance on Model Risk Management}},
year = {2011}
}
@misc{DSIT2024,
author = {{Department for Science Innovation & Technology}},
file = {::},
isbn = {978-1-5286-4565-2},
month = {feb},
title = {{Command Paper: CP 1019. A pro-innovation approach to AI regulation}},
url = {https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach},
urldate = {2024-10-29},
year = {2024}
}
@techreport{UKGovernment2023,
author = {{UK Government}},
file = {::},
keywords = {6.8258,Book,HMG,Management,Orange,Principles: Concepts: Design102,Risk},
title = {{The Orange Book. Management of Risk – Principles and Concepts}},
url = {https://www.gov.uk/government/publications/orange-book},
year = {2023}
}
@techreport{HMTreasury2015,
author = {{HM Treasury}},
file = {::},
isbn = {978-1-910337-67-7},
keywords = {978-1-910337-67-7,PU1740,The Aqua Book: guidance on producing quality analy},
month = {mar},
title = {{The Aqua Book: guidance on producing quality analysis for government}},
url = {https://www.gov.uk/government/publications/the-aqua-book-guidance-on-producing-quality-analysis-for-government},
year = {2015}
}
@article{Perri2023,
abstract = {This letter presents a Convolutional Neural Network (CNN), named WelDeNet, customized to classify welding defects, such as lack of penetration (LP), cracks (CR), porosity (PO) and no defect (ND), by inspecting digitalized radiographic images. A new dataset that collects 24,407 images representing welding defects is also presented. WelDeNet consists of 14 cascaded convolutional layers and achieves a test accuracy of 99.5 %. When hardware implemented within the Raspberry Pi 3B + board, WelDeNet exhibits an inference time of only 134 ms, with CPU and memory utilizations of just 51 % and 47 MB, thus offering a promising solution easy-to-integrate in a real industrial environment.},
author = {Perri, Stefania and Spagnolo, Fanny and Frustaci, Fabio and Corsonello, Pasquale},
doi = {10.1016/j.mfglet.2022.11.006},
file = {::},
issn = {22138463},
journal = {Manufacturing Letters},
keywords = {Convolutional Neural Network,Dataset,Welding defect},
month = {jan},
pages = {29--32},
publisher = {Elsevier Ltd},
title = {{Welding defects classification through a Convolutional Neural Network}},
volume = {35},
year = {2023}
}
@techreport{FSA2009,
author = {{Financial Services Authority}},
file = {::},
title = {{The Turner Review. A regulatory response to the global banking crisis}},
year = {2009}
}
